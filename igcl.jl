# imitation-guided continual learning
using Parameters

@with_kw mutable struct IGCL <: Solver
    Ï€ # Policy
    D # Discriminator
    S::AbstractSpace # State space
    A::AbstractSpace = action_space(Ï€) # Action space
    N::Int = 1000 # Number of environment interactions
    Î”N::Int = 4 # Number of interactions between updates
    max_steps::Int = 100 # Maximum number of steps per episode
    log::Union{Nothing, LoggerParams} = Crux.LoggerParams(;dir="log/prior_er") # The logging parameters
    i::Int = 0 # The current number of environment interactions
    a_opt::Union{Nothing, TrainingParams} = nothing # Training parameters for the actor
    c_opt::TrainingParams = TrainingParams(;loss=Crux.td_loss, name="critic_", epochs=Î”N)# Training parameters for the critic
    d_opt::TrainingParams = TrainingParams(;loss=(D, ğ’Ÿ, ğ’Ÿ_past; info = Dict()) -> begin
            xtilde = vcat(ğ’Ÿ[:s], Flux.onehotbatch(Ï€, ğ’Ÿ[:a]))
            x = vcat(ğ’Ÿ_past[:s], Flux.onehotbatch(Ï€, ğ’Ÿ_past[:a]))
            Lá´°(Crux.GAN_BCELoss(), D, x, xtilde)
        end, name="discriminator_", epochs=1) # Training parameters for the discriminator
        
    # Off-policy-specific parameters
    Ï€â» = deepcopy(Ï€)
    Ï€_explore::Policy = ÏµGreedyPolicy(LinearDecaySchedule(1., 0.1, floor(Int, N/2)), Ï€.outputs) # exploration noise
    target_update = (Ï€â», Ï€; kwargs...) -> polyak_average!(Ï€â», Ï€, 0.005f0) # Function for updating the target network
    target_fn = Crux.DQN_target # Target for critic regression with input signature (Ï€â», ğ’Ÿ, Î³; i)
    buffer_size = 1000 # Size of the buffer
    buffer::ExperienceBuffer = ExperienceBuffer(S, A, buffer_size) # The replay buffer
    buffer_init::Int = max(c_opt.batch_size, 200) # Number of observations to initialize the buffer with
    required_columns = prioritized(buffer) ? [:weight] : Symbol[] # Extra data columns to store
    past_experience::ExperienceBuffer # extra buffers (i.e. for experience replay in continual learning)
end

function POMDPs.solve(ğ’®::IGCL, mdp)
    # Construct the training buffer, constants, and sampler
    ğ’Ÿ = ExperienceBuffer(ğ’®.S, ğ’®.A, ğ’®.c_opt.batch_size, ğ’®.required_columns, device=device(ğ’®.Ï€))
    ğ’Ÿ_past = buffer_like(ğ’®.past_experience, capacity=ğ’®.c_opt.batch_size, device=device(ğ’®.Ï€))
    
    Î³ = Float32(discount(mdp))
    s = Sampler(mdp, ğ’®.Ï€, ğ’®.S, ğ’®.A, max_steps=ğ’®.max_steps, Ï€_explore=ğ’®.Ï€_explore)

    # Log the pre-train performance
    log(ğ’®.log, ğ’®.i, s=s)

    # Fill the buffer with initial observations before training
    ğ’®.i += fillto!(ğ’®.buffer, s, ğ’®.buffer_init, i=ğ’®.i, explore=true)
    
    # Loop over the desired number of environment interactions
    for ğ’®.i in range(ğ’®.i, stop=ğ’®.i + ğ’®.N - ğ’®.Î”N, step=ğ’®.Î”N)
        # Sample transitions into the replay buffer
        data = steps!(s, Nsteps=ğ’®.Î”N, explore=true, i=ğ’®.i)
        data[:r] .+= 0.1f0*sigmoid.(value(ğ’®.D, vcat(data[:s], Flux.onehotbatch(ğ’®.Ï€, data[:a]))))
        push!(ğ’®.buffer, data )
        infos = []
        # Loop over the desired number of training steps
        for epoch in 1:ğ’®.c_opt.epochs
            rand!(ğ’Ÿ, ğ’®.buffer, i=ğ’®.i)
            rand!(ğ’Ÿ_past, ğ’®.past_experience, i=ğ’®.i)
            
            # Train the discriminator
            Crux.train!(ğ’®.D, (;kwargs...)->ğ’®.d_opt.loss(ğ’®.D, ğ’Ÿ, ğ’Ÿ_past), ğ’®.d_opt)
            
            # Sample a new random minibatch
            rand!(ğ’Ÿ, ğ’®.buffer, i=ğ’®.i)
            rand!(ğ’Ÿ_past, ğ’®.past_experience, i=ğ’®.i)
            
            # Compute target
            y = ğ’®.target_fn(ğ’®.Ï€â», ğ’Ÿ, Î³, i=ğ’®.i)
            
            # Update priorities (for prioritized replay)
            (isprioritized = prioritized(ğ’®.buffer)) && update_priorities!(ğ’®.buffer, ğ’Ÿ.indices, cpu(td_error(ğ’®.Ï€, ğ’Ÿ, y)))
            
            # Train the critic
            info = Crux.train!(ğ’®.Ï€, (;kwargs...) -> ğ’®.c_opt.loss(ğ’®.Ï€, ğ’Ÿ, y; weighted=isprioritized, kwargs...), ğ’®.c_opt)
            
            # Train the actor 
            if !isnothing(ğ’®.a_opt) && ((epoch-1) % ğ’®.a_opt.update_every) == 0
                info_a = train!(ğ’®.Ï€.A, (;kwargs...) -> ğ’®.a_opt.loss(ğ’®.Ï€, ğ’Ÿ; kwargs...), ğ’®.a_opt)
                info = merge(info, info_a)
            
                # Update the target network
                ğ’®.target_update(ğ’®.Ï€â», ğ’®.Ï€)
            end
            
            # Store the training information
            push!(infos, info)
            
        end
        # If not using a separate actor, update target networks after critic training
        isnothing(ğ’®.a_opt) && ğ’®.target_update(ğ’®.Ï€â», ğ’®.Ï€, i=ğ’®.i + 1:ğ’®.i + ğ’®.Î”N)
        
        # Log the results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, aggregate_info(infos), s=s)
    end
    ğ’®.i += ğ’®.Î”N
    ğ’®.Ï€
end


function igcl(;default_policy, D, experience_per_task=1000, eval_eps=10, solver_args...)
  (;i, solvers=[], tasks=[], history) -> begin
      # Construct samplers for previous tasks (for recording the new policy performance on previous tasks)
      pol = isempty(solvers) ? default_policy() : deepcopy(solvers[end].Ï€)
      samplers = [Sampler(t, pol, state_space(t)) for t in tasks]
      if i>1
          last_task = tasks[end-1]
          S = state_space(last_task)
          samp = Sampler(last_task, solvers[end].Ï€, S, Ï€_explore=solvers[end].Ï€)  # Sampler for the previous task (swap out with different samplers here)
          new_buffer = ExperienceBuffer(steps!(samp, Nsteps=experience_per_task)) # sample trajectories from that task
          push!(history, new_buffer)
          ğ’Ÿexpert = vcat(history...)
          return IGCL(;Ï€=pol, solver_args..., D=D(), past_experience=ğ’Ÿexpert, log=LoggerParams(dir=string(out,"log/pr_replay/task$i"), fns=[log_undiscounted_return(samplers, eval_eps)]))
      else
          return DQN(;Ï€=pol, solver_args..., log=(dir=string(out,"log/pr_replay/task$i"), fns=[log_undiscounted_return(samplers, eval_eps)]))
      end
  end 
end

